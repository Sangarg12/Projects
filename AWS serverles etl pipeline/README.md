# Hospital JSON âœ Parquet âœ Athena âœ QuickSight ğŸš€

This repo contains an AWS serverless analytics pipeline that:
1. ğŸ“¥ Reads nested hospital JSON files from Amazon S3
2. ğŸ§© Flattens the JSON into a tabular format
3. ğŸ§± Writes the transformed output as Parquet back to S3
4. ğŸ—‚ï¸ Runs an AWS Glue Crawler to create/update an Athena table
5. ğŸ“Š Enables dashboards in Amazon QuickSight using Athena as the source

## Architecture ğŸ—ï¸

- **Amazon S3 (Raw Zone)** ğŸ“¦: Stores incoming JSON files
- **AWS Lambda** âš¡: Triggered on S3 upload; parses, flattens, and converts JSON â†’ Parquet
- **Amazon S3 (Curated Zone)** ğŸ§Š: Stores Parquet outputs (analytics-ready)
- **AWS Glue Crawler + Data Catalog** ğŸ§ : Discovers schema & maintains table metadata for Athena
- **Amazon Athena** ğŸ”: Queries curated Parquet data through the catalog
- **Amazon QuickSight** ğŸ‘ï¸: Builds datasets and dashboards using Athena

## Data Model (Output) ğŸ§¾

**Grain:** 1 row per medicine line item in an order ğŸ’Š

Flattened columns include:

- ğŸ§¾ **Order**: `medicine_order_id`, `order_date`, `order_status`, `order_type`, `priority`
- ğŸ§ **Patient**: `patient_id`, `patient_name`, `gender`, `date_of_birth`
- ğŸ¥ **Encounter**: `encounter_id`, `encounter_type`, `admission_date`, `discharge_date`
- ğŸ§‘â€âš•ï¸ **Doctor**: `doctor_id`, `doctor_name`, `department`
- ğŸ’Š **Medicine**: `medicine_id`, `medicine_name`, `dosage`, `frequency`, `route`, `medicine_start_date`, `medicine_end_date`
- ğŸ¨ **Hospital**: `hospital_id`, `hospital_name`, `location`

## Repository Structure (Suggested) ğŸ—‚ï¸

```
.
â”œâ”€â”€ src/
â”‚   â””â”€â”€ lambda_function.py       âš¡ (Lambda handler + flatten logic)
â”œâ”€â”€ infra/                       ğŸ—ï¸ (optional)
â”‚   â”œâ”€â”€ cloudformation.yaml
â”‚   â””â”€â”€ terraform/
â”œâ”€â”€ sample-data/                 ğŸ§ª (optional)
â”‚   â””â”€â”€ example.json
â”œâ”€â”€ README.md                    ğŸ“˜
â””â”€â”€ CONTRIBUTING.md              ğŸ¤
```

## Prerequisites âœ…

- AWS account with access to: S3, Lambda, Glue, Athena, QuickSight ğŸ”
- S3 bucket with:
  - Raw input prefix (example): `hospital_json_input/` ğŸ“¥
  - Curated output prefix (example): `hospital_parquet_output/` ğŸ“¤
- Glue crawler created (example name used in code): `hospital_json_crawler` ğŸ•·ï¸
- Lambda runtime includes dependencies:
  - `pandas` ğŸ¼
  - `pyarrow` ğŸ¹ (for Parquet writing)
  - `boto3` (included in Lambda runtime)

## Configuration âš™ï¸

Update these based on your environment:

| Parameter | Example Value | Description |
|-----------|---------------|-------------|
| S3 Bucket | `my-hospital-data` | Your S3 bucket name |
| Raw Prefix | `hospital_json_input/` | Where JSON files are uploaded |
| Curated Prefix | `hospital_parquet_output/` | Where Parquet files are written |
| Glue Crawler Name | `hospital_json_crawler` | Crawler that catalogs the data |
| Glue Database | `hospital_analytics` | Glue Data Catalog database name |
| Glue Table | `hospital_orders` | Generated table name (by crawler) |

## How It Works (Step-by-Step) ğŸ”

1. ğŸ“¤ Upload a JSON file to the raw S3 prefix
2. âš¡ S3 event triggers Lambda
3. Lambda performs:
   - ğŸ“– Reads the JSON file
   - ğŸ§© Flattens nested structures into a DataFrame
   - ğŸ§± Writes a timestamped Parquet file to curated prefix (e.g., `hospital_output_20250101_10:30:45.parquet`)
4. ğŸ•·ï¸ Lambda starts the Glue crawler
5. ğŸ—‚ï¸ Glue crawler updates/creates the table in Glue Data Catalog
6. ğŸ” Athena queries the table and QuickSight visualizes it ğŸ“Š

## Deployment (High Level) ğŸ§°

### Step 1: Create S3 Bucket & Prefixes ğŸª£

```bash
# Create bucket (if not exists)
aws s3 mb s3://my-hospital-data --region us-east-1

# Create prefixes
aws s3api put-object --bucket my-hospital-data --key hospital_json_input/
aws s3api put-object --bucket my-hospital-data --key hospital_parquet_output/
```

### Step 2: Create Glue Crawler ğŸ•·ï¸

1. Navigate to AWS Glue Console â†’ Crawlers
2. Create a new crawler named `hospital_json_crawler`
3. Set data source to: `s3://my-hospital-data/hospital_parquet_output/`
4. Create or select Glue database: `hospital_analytics`
5. Output table name: `hospital_orders` (auto-configured)
6. Review and create crawler

### Step 3: Deploy Lambda Function âš¡

1. Create a new Lambda function with Python 3.11 runtime
2. Copy the code from `src/lambda_function.py`
3. Add Lambda layer or include these dependencies in deployment package:
   - `pandas`
   - `pyarrow`
4. Set Lambda execution role with permissions:
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": ["s3:GetObject", "s3:PutObject"],
         "Resource": "arn:aws:s3:::my-hospital-data/*"
       },
       {
         "Effect": "Allow",
         "Action": ["glue:StartCrawler"],
         "Resource": "arn:aws:glue:*:*:crawler/hospital_json_crawler"
       }
     ]
   }
   ```

### Step 4: Configure S3 Event Notification ğŸ””âš¡

1. Go to S3 Bucket â†’ Properties â†’ Event Notifications
2. Create event for ObjectCreated â†’ Lambda â†’ Select your Lambda function
3. Set filter prefix to: `hospital_json_input/`
4. Save

### Step 5: Validate in Athena ğŸ”

```sql
-- Check if table exists
SHOW TABLES IN hospital_analytics;

-- Run sample query
SELECT COUNT(*) as total_medicine_records
FROM hospital_analytics.hospital_orders;

-- Check schema
DESCRIBE hospital_analytics.hospital_orders;
```

### Step 6: Create QuickSight Dataset & Dashboards ğŸ“Š

1. Open Amazon QuickSight
2. Create new dataset â†’ Select Athena
3. Choose database: `hospital_analytics` and table: `hospital_orders`
4. Proceed to build visuals (charts, KPIs, filters, etc.)
5. Publish dashboard

## Validation Checklist ğŸ§ª

- âœ… Parquet files appear in curated prefix after JSON upload
- âœ… Glue crawler run succeeds and table schema is correct
- âœ… Athena returns expected row counts (medicine-line item level)
- âœ… QuickSight dataset refresh works and visuals load correctly
- âœ… Row count validation: Total output rows = sum of medicines across all source records

## Common Pitfalls âš ï¸

| Issue | Solution |
|-------|----------|
| ğŸ§¨ Mixed formats in one S3 location | Keep raw JSON and curated Parquet in separate prefixes |
| ğŸ§¬ Schema drift after JSON structure changes | Monitor crawler logs and manually update schema in Glue if needed |
| ğŸ“¦ Lambda timeout/memory error with `pyarrow` | Use Lambda layers (~200MB+) or container images for large dependencies |
| ğŸ” Permission denied errors | Verify Lambda IAM role has S3 + Glue permissions |
| ğŸ•·ï¸ Crawler fails to infer schema | Check S3 prefix contains valid Parquet files; crawler logs in CloudWatch |

## Future Improvements ğŸŒ±

- ğŸ“… **Partitioning**: Partition Parquet by `order_date` for faster Athena queries
- âœ… **Data Quality**: Add validation checks (null counts, duplicates, referential integrity)
- ğŸ§­ **Orchestration**: Use AWS Step Functions for retries, error handling, and monitoring
- â™»ï¸ **Incremental Processing**: Implement delta/incremental load strategy instead of full loads
- ğŸ”„ **Schema Evolution**: Handle schema changes (e.g., new columns in JSON)
- ğŸ“ˆ **Performance**: Optimize Parquet compression (`snappy` vs `gzip`) and file sizes

## Testing Locally ğŸ§ª

```python
# Test the flatten function locally
import json
from lambda_function import flatten

with open('sample-data/example.json', 'r') as f:
    data = json.load(f)

df = flatten(data)
print(df.head())
print(f"Total rows: {len(df)}")
print(f"Columns: {df.columns.tolist()}")

# Optionally save to local Parquet for inspection
df.to_parquet('test_output.parquet')
```

## Contributing ğŸ¤

1. Create a feature branch: `git checkout -b feature/your-feature`
2. Commit changes: `git commit -m "Add your message"`
3. Push to branch: `git push origin feature/your-feature`
4. Open a Pull Request

## Troubleshooting ğŸ”§

### Lambda doesn't trigger on S3 upload
- Verify event notification is configured on correct bucket prefix
- Check Lambda execution role has S3 permissions
- Review CloudWatch logs for errors

### Glue crawler fails
- Check S3 path exists and contains valid Parquet files
- Verify crawler IAM role has S3 read permissions
- Review crawler logs in AWS Glue console

### Athena query fails or returns no results
- Ensure Glue table was created/updated by crawler
- Run `SHOW PARTITIONS table_name;` if using partitioning
- Check Athena query results in S3 (Athena writes query results to a results bucket)

### QuickSight can't connect to Athena
- Verify QuickSight has Athena permissions and S3 results bucket access
- Check QuickSight region matches Athena region
- Test Athena query directly before using in QuickSight

## License ğŸ“œ

This project is licensed under the MIT License - see LICENSE file for details

## Support ğŸ’¬

For issues, questions, or suggestions:
- Open a GitHub Issue
- Check existing issues for similar problems
- Include error logs and AWS service outputs when reporting bugs

---

**Last Updated:** December 2025  
**Maintained By:** Analytics Team  
**Status:** Active âœ…
